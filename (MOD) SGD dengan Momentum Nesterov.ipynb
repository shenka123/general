{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD) Dengan Momentum Nesterov\n",
    "### Modifikasi dengan Optimizer **ADAM**\n",
    "Benediktus Sashenka\n",
    "10117080\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *   \n",
    "import numpy as np\n",
    "from numpy.random import *\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision = 3, suppress = True, formatter = {'float':'{:5.4f}'.format})\n",
    "\n",
    "#Sigmoid function & its derivative\n",
    "sigmoid = lambda Z: 1/(1+exp(-Z))\n",
    "dsigmoid = lambda A: A*(1-A)\n",
    "\n",
    "#ReLU function & its derivative\n",
    "ReLU  = lambda Z: Z.clip(0)\n",
    "#Derivative of ReLU function\n",
    "dReLU = lambda A: (A > 0)*1\n",
    "\n",
    "#Derivativer of tanh()\n",
    "dtanh = lambda A: 1-A**2\n",
    "\n",
    "#Derivative oh arctanh\n",
    "darctanh = lambda A: 1/(A**2+1)\n",
    "\n",
    "#Softplus function & its derivative\n",
    "splus = lambda Z: log(1+exp(Z))\n",
    "dsplus = lambda A: 1/(1+exp(-A))\n",
    "\n",
    "linear = lambda X,w,b: X@w+b\n",
    "\n",
    "\"Time step (ts)\"\n",
    "def steps(x, step):   \n",
    "    obs  = len(x)-step\n",
    "    xt   = x[:obs,:]\n",
    "    for i in arange(1,step+1):\n",
    "        xt = hstack((xt, x[i:obs+i,:]))   \n",
    "    return xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifikasi pada kelas NeuralNetwork\n",
    "\n",
    "Pada ANN biasa, dengan *back propagation* sudah dapat diketahui nilai dari \n",
    "$\\frac{\\partial C}{\\partial W}$, $\\frac{\\partial C}{\\partial b}$\n",
    "untuk parameter $W, b$ pada setiap hidden layer. \n",
    "\n",
    "Optimizer **ADAM** menyimpan $m$ dan $v$ sebagai estimator momen pertama dan kedua untuk diingat,\n",
    "\\begin{align}\n",
    "m_t &= \\beta_1  m_{t-1} + (1-\\beta_1 )g_t \\\\\n",
    "v_t &= \\beta_2  v_{t-1} + (1-\\beta_2 )g_t^2\n",
    "\\end{align}\n",
    "dengan $g_t$ adalah gradien descend, $\\beta_1$ dan $\\beta_2$ adalah decay rates. \n",
    "\n",
    "Namun karena estimator $m$ dan $v$ bias, maka harus dilakukan koreksi estimator untuk menghilangkan kebiasan\n",
    "\\begin{equation}\n",
    "\\hat{m_t} = \\frac{m_t}{1-\\beta_1^t} ,\\\n",
    "\\hat{v_t} = \\frac{v_t}{1-\\beta_2^t}\n",
    "\\end{equation}\n",
    "\n",
    "Selanjutnya dapat dilakukan proses SGD dengan ADAM yaitu\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v_t}}+\\epsilon} \\hat{m_t}\n",
    "\\end{equation}\n",
    "dengan $\\alpha$ learning rate dan $\\epsilon$ untuk mencegah pembagian dengan 0. \n",
    "\n",
    "Ambil $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, dan $\\epsilon = 10^{-8}$, lalu dapat diterapkan proses SGD dengan ADAM untuk parameter $W$ dan $b$ sehingga\n",
    "\n",
    "\\begin{align}\n",
    "m_W(t) &= \\beta_1  m_W(t-1) + (1-\\beta_1 )\\frac{\\partial C}{\\partial W}, &\n",
    "m_b(t) &= \\beta_1  m_b(t-1) + (1-\\beta_1 )\\frac{\\partial C}{\\partial b} \\\\\n",
    "v_W(t) &= \\beta_2  v_W(t-1) + (1-\\beta_2 )\\left(\\frac{\\partial C}{\\partial W}\\right)^2, &\n",
    "v_b(t) &= \\beta_2  v_b(t-1) + (1-\\beta_2 )\\left(\\frac{\\partial C}{\\partial b}\\right)^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Lalu dilakukan koreksi estimator\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{m_W} &= \\frac{m_W(t)}{1-\\beta_1^t} , & \\hat{m_b} &= \\frac{m_b(t)}{1-\\beta_1^t} \\\\\n",
    "\\hat{v_W} &= \\frac{v_W(t)}{1-\\beta_2^t} , & \\hat{v_b} &= \\frac{v_b(t)}{1-\\beta_2^t} \n",
    "\\end{align}\n",
    "\n",
    "Setelah mendapat estimator untuk momen, parameter $W$ dan $b$ dapat diperbarui\n",
    "\\begin{align}\n",
    "W(t+1) &= W(t) - \\frac{\\alpha}{\\sqrt{\\hat{v_W}}+\\epsilon} \\hat{m_W} \\\\\n",
    "b(t+1) &= b(t) - \\frac{\\alpha}{\\sqrt{\\hat{v_b}}+\\epsilon} \\hat{m_b} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Lalu karena sudah memakai momentum optimizer ADAM, learning rate yang digunakan tidak terlalu besar yaitu 0.001.\n",
    "\n",
    "Catatan: kuadrat dan akar pada operasi matriks dilakukan elemen per elemen (e.g. [2, 3]^2 = [4, 9])\n",
    "\n",
    "Referensi :\n",
    "Ruder, S. 2016. An overview of gradient descent optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,x,y,hlayers,alpha,mbs): # ada h nodes di dalam hidden layer  hlayers = [7, 3, 34, 89]\n",
    "        self.X  = x   #input\n",
    "        self.y  = y   #output\n",
    "        self.hlayers = hlayers   #note: untuk model Perceptron gunakan hlayers = []\n",
    "        self.α  = alpha\n",
    "        self.mbs = mbs\n",
    "        \n",
    "        #self.Xt = x   #akan digunakan untuk SGD\n",
    "        #self.yt = y   #akan digunakisn di SGD\n",
    "        \n",
    "        assert ndim(y)            == 2     #y harus berupa matriks (berdimensi 2)\n",
    "        assert type(self.hlayers) == list  #hlayers harus berupa sebuah list\n",
    "        \n",
    "        self.N, ni = shape(x)   #jumlah features (variables)\n",
    "        self.N, no = shape(y)   #jumlah observasi (self.N) dan jumlah output (no)\n",
    "        \n",
    "        self.neurons = [ni]\n",
    "        self.neurons.extend(self.hlayers)\n",
    "        self.neurons.append(no)   #jumlah neurons per layer, termasuk input layer \n",
    "        self.nlayers  = len(self.neurons)-1  #number of layers (tidak termasuk input layer)\n",
    "\n",
    "        \"Initial values untuk parameter w and b\"\n",
    "\n",
    "        self.w, self.b = [], []\n",
    "        for i in arange(self.nlayers):\n",
    "            self.w.append(randn(self.neurons[i], self.neurons[i+1]))  #Sinapsis dari layer ke i menuju layer ke (i+1)  \n",
    "            self.b.append(randn(1, self.neurons[i+1]))           #Bias di layer ke (i+1)    \n",
    "          \n",
    "        \"Initial values untuk momen dengan random, momen v harus positif\"\n",
    "        \n",
    "        self.Mw, self.Mb = [], []\n",
    "        for i in arange(self.nlayers):\n",
    "            self.Mw.append(randn(self.neurons[i], self.neurons[i+1]))  \n",
    "            self.Mb.append(randn(1, self.neurons[i+1]))  \n",
    "            \n",
    "        self.Vw, self.Vb = [], []\n",
    "        for i in arange(self.nlayers):\n",
    "            self.Vw.append(randn(self.neurons[i], self.neurons[i+1])**2)  \n",
    "            self.Vb.append(randn(1, self.neurons[i+1])**2)  \n",
    "\n",
    "    def training(self):\n",
    "        \n",
    "        acak  = choice(len(self.y), int(self.mbs*len(self.y)), replace = False)   #bikin indeks pengambilan secara acak sebesar bs\n",
    "        Xmini = self.X[acak]   #mini batch sample dari X\n",
    "        ymini = self.y[acak]   #mini batch sample dari y\n",
    "        \n",
    "        epoch = 0 #menyimpan jumlah iterasi\n",
    "        \n",
    "        \n",
    "        for Xt,yt in zip(Xmini,ymini):\n",
    "        \n",
    "            \"Forward propagation (perambatan maju)\"\n",
    "    \n",
    "            Z, A = [], [Xt]\n",
    "        \n",
    "            \"Dimulai dari Hidden layers\"\n",
    "            for j in arange(len(hlayers)):   #hanya hidden layers\n",
    "                Z.append(linear(A[j],self.w[j],self.b[j]))  #Reaksi kimia di layer ke (j + 1)\n",
    "                A.append(tanh(Z[j]))     #Aliran listrik di layer ke (j + 1)\n",
    "        \n",
    "            \"Output layer\"\n",
    "            Z.append(linear(A[self.nlayers-1],self.w[self.nlayers-1],self.b[self.nlayers-1]))  #Reaksi kimia di output layer\n",
    "            A.append(sigmoid(Z[self.nlayers-1]))\n",
    "    \n",
    "            self.predicted_yt = A[self.nlayers]\n",
    "            e         = yt-self.predicted_yt\n",
    "\n",
    "            \"Backward propagation (perambatan mundur)\"\n",
    "        \n",
    "            \n",
    "            \"Dimulai dari output layer\"\n",
    "            dCdZ = [(-2*e/len(yt))*dsigmoid(A[::-1][0])]  #berbentuk list agar bisa di-append, mulai dari output layer\n",
    "            \n",
    "            beta = 0.1   #0.02\n",
    "            \"Hidden layer\"                                           \n",
    "            for m in arange(self.nlayers-1):  #dan mundur ke layer berikutnya, sampai hidden layer pertama\n",
    "                dCdZ.append((dCdZ[m]@(self.w[::-1][m]-beta*self.Vw[::-1][m]).T)*dtanh(A[::-1][m+1]))   #delta\n",
    "        \n",
    "            \"Perubahan parameters (w dan b):\"\n",
    "    \n",
    "            one  = ones([1,len(yt)])\n",
    "            dCdw, dCdb = [], []   #dalam bentuk list agar bisa di-append\n",
    "            \n",
    "            \n",
    "            \"_____________________ MODIFIKASI DENGAN ADAM _____________________\"\n",
    "            \n",
    "            \"Parameter ADAM\"\n",
    "            \n",
    "            t     = epoch\n",
    "            beta1 = 0.9\n",
    "            beta2 = 0.999\n",
    "            eps   = 1E-8\n",
    "            \n",
    "            \"Stochastic Gradient Descents\"\n",
    "            \n",
    "            for n in arange(self.nlayers): \n",
    "                \n",
    "                \"Meghitung gradien C terhadap W dan b, dengan menggunakan dC/dZ\"\n",
    "                \n",
    "                dCdw.append(A[n].T@dCdZ[::-1][n])  \n",
    "                dCdb.append(one@dCdZ[::-1][n])     \n",
    "                \n",
    "                \"Menghitung estimator momen m dan v\"\n",
    "                \n",
    "                self.Mw[n] = beta1*array(self.Mw[n]) + (1-beta1)*array(dCdw[n])\n",
    "                self.Mb[n] = beta1*array(self.Mb[n]) + (1-beta1)*array(dCdb[n])\n",
    "                \n",
    "                self.Vw[n] = beta2*array(self.Vw[n]) + (1-beta2)*array(dCdw[n])**2\n",
    "                self.Vb[n] = beta2*array(self.Vb[n]) + (1-beta2)*array(dCdb[n])**2\n",
    "                \n",
    "                \"Menghilangkan kebiasan m dan v\"\n",
    "                \n",
    "                Mw_est = array(self.Mw[n])/(1-beta1**(t+1))\n",
    "                Mb_est = array(self.Mb[n])/(1-beta1**(t+1))\n",
    "                Vw_est = array(self.Vw[n])/(1-beta2**(t+1))\n",
    "                Vb_est = array(self.Vb[n])/(1-beta2**(t+1))\n",
    "                \n",
    "                \"Updating parameter W dan b\"\n",
    "                \n",
    "                self.w[n] -= (self.α/(np.sqrt(Vw_est)+eps))*Mw_est\n",
    "                self.b[n] -= (self.α/(np.sqrt(Vb_est)+eps))*Mb_est\n",
    "                \n",
    "                \n",
    "            \"_____________________ MODIFIKASI DENGAN ADAM _____________________\"\n",
    "            \n",
    "            epoch += 1\n",
    "            \n",
    "    \n",
    "    def prediction(self, Xs, ys):   #Gunakan data secara keseluruhan\n",
    "        \n",
    "        \"Forward propagation (perambatan maju)\"\n",
    "    \n",
    "        Z, A = [], [Xs]\n",
    "        \n",
    "        \"Dimulai dari Hidden layers\"\n",
    "        for j in arange(len(hlayers)):   #hanya hidden layers\n",
    "            Z.append(linear(A[j],self.w[j],self.b[j]))  #Reaksi kimia di layer ke (j + 1)\n",
    "            A.append(tanh(Z[j]))     #Aliran listrik di layer ke (j + 1)\n",
    "        \n",
    "        \"Output layer\"\n",
    "        Z.append(linear(A[self.nlayers-1],self.w[self.nlayers-1],self.b[self.nlayers-1]))  #Reaksi kimia di output layer\n",
    "        A.append(sigmoid(Z[self.nlayers-1]))\n",
    "    \n",
    "        self.predicted_y = A[self.nlayers]\n",
    "        e         = ys-self.predicted_y\n",
    "        self.Cost = e.T@e/len(ys)  #mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediksi harga saham ASII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A  = pd.read_csv('ASII_10117080.csv')  #Data time series harian harga saham ASII\n",
    "A['Adj Close'].fillna(A['Adj Close'].mean(), inplace=True)\n",
    "A6 = np.array(A['Adj Close'])\n",
    "B  = A6[:,newaxis]  #berupa matriks\n",
    "\n",
    "Bmin = min(B)\n",
    "Bmax = max(B)\n",
    "b = (B-Bmin)/(Bmax-Bmin)\n",
    "\n",
    "ts = 1   #Di literatur Time Series digunakan istilah 'lag' sebagai padanan istilah 'timestep' ini\n",
    "xt = steps(b, ts)  #Dihasilkan matriks dengan 2 (= ts+1) kolom, kolom pertama menjadi variabel X\n",
    "                   #dan kolom terkhir menjadi variabel y\n",
    "\n",
    "x = xt[-800:,:]    #Ambil 800 observasi terakhir\n",
    "\n",
    "#Data untuk training\n",
    "Xtrain = x[0:680, :-1]   #Ambil 680 observasi yang pertama dan hilangkan kolom terakhir\n",
    "ytrain = x[0:680:, -1:]  #Ambil 680 observasi yang pertama dan ambil kolom terakhir sebagai variabel y \n",
    "\n",
    "#Data untuk testing\n",
    "Xtest = x[680:, :-1]   #ambil jumlah observasi sebanyak 120, hilangkan kolom terakhir (untuk y)\n",
    "ytest = x[680:, -1:]   #ambil kolom terakhir\n",
    "\n",
    "hlayers = [3, 2]  #Dua hidden layers, masing-masing dengan 3 neurons dan 2 neurons\n",
    "alpha   = 0.001\n",
    "mbs     = 0.15  #hanya 15% dari seluruh observasi yang digunakan untuk penaksiran parameter\n",
    "epochs  = 500\n",
    "\n",
    "#Rangkuman activation function di hidden layers & output layer\n",
    "\n",
    "#tanh & sigmoid:\n",
    "#SGD dengan Momentum: mbs = 0.15, alpha = 0.65, beta = 0.1, epochs = 400 initial values di luar epochs\n",
    "#SGD : mbs = 1,    alpha = 0.17,  epochs = 70, \n",
    "#MBGD: mbs = 0.15, alpha = 0.165, epochs = 400, alpha = 0.27, epochs = 1600\n",
    "#      mbs = 0.20, alpha = 0.165, epochs = 450\n",
    "\n",
    "tic = datetime.now()\n",
    "\n",
    "seed(20200219)\n",
    "\n",
    "ann2 = NeuralNetwork(Xtrain,ytrain,hlayers,alpha,mbs)   #pembuatan object ann2\n",
    "\n",
    "for t in range(epochs): \n",
    "    ann2.training()\n",
    "    ann2.prediction(Xtrain,ytrain)   #Gunakan data untuk training secara keseluruhan\n",
    "    \n",
    "    if (t+1)%(epochs/10) == 0:   #tampilkan output lima kali\n",
    "        print(\"Sampai epoch ke\", t+1,\"dicapai akurasi MSE sebesar %8.7f\" %ann2.Cost,\"dengan waktu\", datetime.now()-tic)\n",
    "\n",
    "toc = datetime.now()\n",
    "print('\\nAkurasi dengan data training: %8.7f' %ann2.Cost,'\\n')\n",
    "\n",
    "plt.plot(ytrain, label = 'actual series')\n",
    "plt.plot(ann2.predicted_y, label = 'predicted series')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "toc = datetime.now()\n",
    "print(f'Waktu yang diperlukan dari mulai training ({epochs} epochs): ', toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model Deep Learning dengan data di periode yang lain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters w0, w1 dan w2 sebagai ilmu pengetahuan atau knowledge yang diperoleh selama training \n",
    "#akan digunakan disini untuk di-test kemampuannya memprediksi. \n",
    "#Data yang dipakai Xtest dan ytest yang tidak digunakan selama training.\n",
    "\n",
    "Cost_train = ann2.Cost    #Ini untuk data untuk training\n",
    "\n",
    "ann2.prediction(Xtest,ytest)   #Gunakan data untuk test\n",
    "predicted_ytest = ann2.predicted_y\n",
    "Cost_test = ann2.Cost          #Ini dengan data untuk test \n",
    "\n",
    "print('\\nMSE training: %8.7f'%Cost_train,'\\nMSE testing : %8.7f'%Cost_test)\n",
    "\n",
    "ytesto    = ytest*(Bmax-Bmin)+Bmin      #Kembalikan datanya ke original unit dalam satuan dolar\n",
    "ytesthato = predicted_ytest*(Bmax-Bmin)+Bmin   #Kembalikan datanya ke original unit dalam satuan dolar\n",
    "\n",
    "plt.plot(ytesto, label = 'actual series')\n",
    "plt.plot(ytesthato, label = 'predicted series')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "toc = datetime.now()\n",
    "\n",
    "print(f'Waktu untuk training (dengan {epochs} epochs) dan prediksi: ',  toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
